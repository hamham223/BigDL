# BigDL Orca Distribute Workflow
apiVersion: batch/v1
kind: Job
metadata:
  name: bigdl-orca-ncf
spec:
  template:
    spec:
      restartPolicy: OnFailure
      containers:
      - name: bigdl-distribute-workflow
        image: intelanalytics/bigdl-orca:latest
        imagePullPolicy: IfNotPresent
        volumeMounts:
        - mountPath: /output
          name: nfs-storage
        env:
        - name: http_proxy
          value: {{ .Values.httpProxy }}
        - name: https_proxy
          value: {{ .Values.httpsProxy }}
        - name: no_proxy
          value: {{ .Values.noProxy }}
        command: ["/bin/bash", "-c"]
        args:
          - cd /output/yinchen;
            mkdir bigdl-distribute-workflow;
            cd bigdl-distribute-workflow;
            echo "Download dataset";
            wget "https://files.grouplens.org/datasets/movielens/ml-100k.zip" -O ml-100k.zip;
            unzip -o ./ml-100k.zip;
            echo "Install required software";
            cd -;
            git clone "https://github.com/intel-analytics/BigDL.git";
            cd BigDL/python/orca/tutorial/NCF;
            pip install tensorflow==2.9.0;
            echo "Start distributed training";
            python ./tf_train_spark_dataframe.py --dataset ml-100k --data_dir /output/yinchen/bigdl-distribute-workflow --model_dir /output/yinchen/bigdl-distribute-workflow;
            echo "Start distributed inference";
            python ./tf_predict_spark_dataframe.py --dataset ml-100k --data_dir /output/yinchen/bigdl-distribute-workflow --model_dir /output/yinchen/bigdl-distribute-workflow;
            echo "Orca NCF tutorial is completed successfully";
            exit;
      volumes:
      - name: nfs-storage
        persistentVolumeClaim:
          claimName: nfsvolumeclaim
